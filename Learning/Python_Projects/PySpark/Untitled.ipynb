{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "buried-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Sample\").config(\"spark.driver.memory\",\"4g\").config(\"spark.executor.memory\",\"4g\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "focal-piece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Sample</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20e002dd3c8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incorporate-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores=spark._jsc.sc().getExecutorMemoryStatus().keySet().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "southern-mason",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mighty-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extensive-jumping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Learning\\Python_Projects\\PySpark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "caroline-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = spark.read.csv(\"./fire-incidents.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "steady-montreal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541014"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "latter-alberta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataFrame.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "timely-german",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IncidentNumber: string (nullable = true)\n",
      " |-- ExposureNumber: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- IncidentDate: string (nullable = true)\n",
      " |-- CallNumber: string (nullable = true)\n",
      " |-- AlarmDtTm: string (nullable = true)\n",
      " |-- ArrivalDtTm: string (nullable = true)\n",
      " |-- CloseDtTm: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- ZIPCode: string (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- SuppressionUnits: integer (nullable = true)\n",
      " |-- SuppressionPersonnel: integer (nullable = true)\n",
      " |-- EMSUnits: integer (nullable = true)\n",
      " |-- EMSPersonnel: integer (nullable = true)\n",
      " |-- OtherUnits: integer (nullable = true)\n",
      " |-- OtherPersonnel: integer (nullable = true)\n",
      " |-- FirstUnitOnScene: string (nullable = true)\n",
      " |-- EstimatedPropertyLoss: integer (nullable = true)\n",
      " |-- EstimatedContentsLoss: string (nullable = true)\n",
      " |-- FireFatalities: string (nullable = true)\n",
      " |-- FireInjuries: string (nullable = true)\n",
      " |-- CivilianFatalities: integer (nullable = true)\n",
      " |-- CivilianInjuries: integer (nullable = true)\n",
      " |-- NumberofAlarms: integer (nullable = true)\n",
      " |-- PrimarySituation: string (nullable = true)\n",
      " |-- MutualAid: string (nullable = true)\n",
      " |-- ActionTakenPrimary: string (nullable = true)\n",
      " |-- ActionTakenSecondary: string (nullable = true)\n",
      " |-- ActionTakenOther: string (nullable = true)\n",
      " |-- DetectorAlertedOccupants: string (nullable = true)\n",
      " |-- PropertyUse: string (nullable = true)\n",
      " |-- AreaofFireOrigin: string (nullable = true)\n",
      " |-- IgnitionCause: string (nullable = true)\n",
      " |-- IgnitionFactorPrimary: string (nullable = true)\n",
      " |-- IgnitionFactorSecondary: string (nullable = true)\n",
      " |-- HeatSource: string (nullable = true)\n",
      " |-- ItemFirstIgnited: string (nullable = true)\n",
      " |-- HumanFactorsAssociatedwithIgnition: string (nullable = true)\n",
      " |-- StructureType: string (nullable = true)\n",
      " |-- StructureStatus: string (nullable = true)\n",
      " |-- FloorofFireOrigin: integer (nullable = true)\n",
      " |-- FireSpread: string (nullable = true)\n",
      " |-- NoFlameSpead: string (nullable = true)\n",
      " |-- Numberoffloorswithminimumdamage: integer (nullable = true)\n",
      " |-- Numberoffloorswithsignificantdamage: integer (nullable = true)\n",
      " |-- Numberoffloorswithheavydamage: integer (nullable = true)\n",
      " |-- Numberoffloorswithextremedamage: integer (nullable = true)\n",
      " |-- DetectorsPresent: string (nullable = true)\n",
      " |-- DetectorType: string (nullable = true)\n",
      " |-- DetectorOperation: string (nullable = true)\n",
      " |-- DetectorEffectiveness: string (nullable = true)\n",
      " |-- DetectorFailureReason: string (nullable = true)\n",
      " |-- AutomaticExtinguishingSystemPresent: string (nullable = true)\n",
      " |-- AutomaticExtinguishingSytemType: string (nullable = true)\n",
      " |-- AutomaticExtinguishingSytemPerfomance: string (nullable = true)\n",
      " |-- AutomaticExtinguishingSytemFailureReason: string (nullable = true)\n",
      " |-- NumberofSprinklerHeadsOperating: integer (nullable = true)\n",
      " |-- SupervisorDistrict: integer (nullable = true)\n",
      " |-- AnalysisNeighborhood: string (nullable = true)\n",
      " |-- point: string (nullable = true)\n",
      " |-- NeighborhoodsOld: integer (nullable = true)\n",
      " |-- ZipCodes: integer (nullable = true)\n",
      " |-- FirePreventionDistricts: integer (nullable = true)\n",
      " |-- PoliceDistricts: integer (nullable = true)\n",
      " |-- SupervisorDistricts: integer (nullable = true)\n",
      " |-- CivicCenterHarmReductionProjectBoundary: integer (nullable = true)\n",
      " |-- 2017FixItZones: integer (nullable = true)\n",
      " |-- HSOCZones: integer (nullable = true)\n",
      " |-- CentralMarketTenderloinBoundary: integer (nullable = true)\n",
      " |-- CentralMarketTenderloinBoundaryPolygonUpdated: integer (nullable = true)\n",
      " |-- HSOCZonesasof20180605: integer (nullable = true)\n",
      " |-- Neighborhoods: integer (nullable = true)\n",
      " |-- SFFindNeighborhoods: integer (nullable = true)\n",
      " |-- CurrentPoliceDistricts: integer (nullable = true)\n",
      " |-- CurrentSupervisorDistricts: integer (nullable = true)\n",
      " |-- AnalysisNeighborhoods: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "lesbian-conspiracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\softwares\\python 3.6\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IncidentNumber</th>\n",
       "      <th>ExposureNumber</th>\n",
       "      <th>ID</th>\n",
       "      <th>Address</th>\n",
       "      <th>IncidentDate</th>\n",
       "      <th>CallNumber</th>\n",
       "      <th>AlarmDtTm</th>\n",
       "      <th>ArrivalDtTm</th>\n",
       "      <th>CloseDtTm</th>\n",
       "      <th>City</th>\n",
       "      <th>...</th>\n",
       "      <th>2017FixItZones</th>\n",
       "      <th>HSOCZones</th>\n",
       "      <th>CentralMarketTenderloinBoundary</th>\n",
       "      <th>CentralMarketTenderloinBoundaryPolygonUpdated</th>\n",
       "      <th>HSOCZonesasof20180605</th>\n",
       "      <th>Neighborhoods</th>\n",
       "      <th>SFFindNeighborhoods</th>\n",
       "      <th>CurrentPoliceDistricts</th>\n",
       "      <th>CurrentSupervisorDistricts</th>\n",
       "      <th>AnalysisNeighborhoods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20104668</td>\n",
       "      <td>0</td>\n",
       "      <td>201046680</td>\n",
       "      <td>MARIPOSA STREET</td>\n",
       "      <td>2020-09-11T00:00:00.000</td>\n",
       "      <td>202550069</td>\n",
       "      <td>2020-09-11T00:53:28.000</td>\n",
       "      <td>2020-09-11T00:58:28.000</td>\n",
       "      <td>2020-09-11T01:27:31.000</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20104708</td>\n",
       "      <td>0</td>\n",
       "      <td>201047080</td>\n",
       "      <td>355 27TH STREET</td>\n",
       "      <td>2020-09-11T00:00:00.000</td>\n",
       "      <td>202550443</td>\n",
       "      <td>2020-09-11T06:48:06.000</td>\n",
       "      <td>2020-09-11T06:49:52.000</td>\n",
       "      <td>2020-09-11T07:07:16.000</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  IncidentNumber ExposureNumber         ID          Address  \\\n",
       "0       20104668              0  201046680  MARIPOSA STREET   \n",
       "1       20104708              0  201047080  355 27TH STREET   \n",
       "\n",
       "              IncidentDate CallNumber                AlarmDtTm  \\\n",
       "0  2020-09-11T00:00:00.000  202550069  2020-09-11T00:53:28.000   \n",
       "1  2020-09-11T00:00:00.000  202550443  2020-09-11T06:48:06.000   \n",
       "\n",
       "               ArrivalDtTm                CloseDtTm           City  ...  \\\n",
       "0  2020-09-11T00:58:28.000  2020-09-11T01:27:31.000  San Francisco  ...   \n",
       "1  2020-09-11T06:49:52.000  2020-09-11T07:07:16.000  San Francisco  ...   \n",
       "\n",
       "  2017FixItZones HSOCZones CentralMarketTenderloinBoundary  \\\n",
       "0            NaN       NaN                             NaN   \n",
       "1            NaN       NaN                             NaN   \n",
       "\n",
       "  CentralMarketTenderloinBoundaryPolygonUpdated  HSOCZonesasof20180605  \\\n",
       "0                                           NaN                    NaN   \n",
       "1                                           NaN                    NaN   \n",
       "\n",
       "   Neighborhoods  SFFindNeighborhoods  CurrentPoliceDistricts  \\\n",
       "0             54                   54                       2   \n",
       "1             84                   84                       9   \n",
       "\n",
       "   CurrentSupervisorDistricts  AnalysisNeighborhoods  \n",
       "0                           9                     26  \n",
       "1                           5                     22  \n",
       "\n",
       "[2 rows x 80 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "induced-butter",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = dataFrame.withColumn(\"IncidentNumber\",col(\"IncidentNumber\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame.set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "thousand-holmes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- ExposureNumber: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- IncidentDate: string (nullable = true)\n",
      " |-- CallNumber: string (nullable = true)\n",
      " |-- AlarmDtTm: string (nullable = true)\n",
      " |-- ArrivalDtTm: string (nullable = true)\n",
      " |-- CloseDtTm: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- ZIPCode: string (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- SuppressionUnits: integer (nullable = true)\n",
      " |-- SuppressionPersonnel: integer (nullable = true)\n",
      " |-- EMSUnits: integer (nullable = true)\n",
      " |-- EMSPersonnel: integer (nullable = true)\n",
      " |-- OtherUnits: integer (nullable = true)\n",
      " |-- OtherPersonnel: integer (nullable = true)\n",
      " |-- FirstUnitOnScene: string (nullable = true)\n",
      " |-- EstimatedPropertyLoss: integer (nullable = true)\n",
      " |-- EstimatedContentsLoss: string (nullable = true)\n",
      " |-- FireFatalities: string (nullable = true)\n",
      " |-- FireInjuries: string (nullable = true)\n",
      " |-- CivilianFatalities: integer (nullable = true)\n",
      " |-- CivilianInjuries: integer (nullable = true)\n",
      " |-- NumberofAlarms: integer (nullable = true)\n",
      " |-- PrimarySituation: string (nullable = true)\n",
      " |-- MutualAid: string (nullable = true)\n",
      " |-- ActionTakenPrimary: string (nullable = true)\n",
      " |-- ActionTakenSecondary: string (nullable = true)\n",
      " |-- ActionTakenOther: string (nullable = true)\n",
      " |-- DetectorAlertedOccupants: string (nullable = true)\n",
      " |-- PropertyUse: string (nullable = true)\n",
      " |-- AreaofFireOrigin: string (nullable = true)\n",
      " |-- IgnitionCause: string (nullable = true)\n",
      " |-- IgnitionFactorPrimary: string (nullable = true)\n",
      " |-- IgnitionFactorSecondary: string (nullable = true)\n",
      " |-- HeatSource: string (nullable = true)\n",
      " |-- ItemFirstIgnited: string (nullable = true)\n",
      " |-- HumanFactorsAssociatedwithIgnition: string (nullable = true)\n",
      " |-- StructureType: string (nullable = true)\n",
      " |-- StructureStatus: string (nullable = true)\n",
      " |-- FloorofFireOrigin: integer (nullable = true)\n",
      " |-- FireSpread: string (nullable = true)\n",
      " |-- NoFlameSpead: string (nullable = true)\n",
      " |-- Numberoffloorswithminimumdamage: integer (nullable = true)\n",
      " |-- Numberoffloorswithsignificantdamage: integer (nullable = true)\n",
      " |-- Numberoffloorswithheavydamage: integer (nullable = true)\n",
      " |-- Numberoffloorswithextremedamage: integer (nullable = true)\n",
      " |-- DetectorsPresent: string (nullable = true)\n",
      " |-- DetectorType: string (nullable = true)\n",
      " |-- DetectorOperation: string (nullable = true)\n",
      " |-- DetectorEffectiveness: string (nullable = true)\n",
      " |-- DetectorFailureReason: string (nullable = true)\n",
      " |-- AutomaticExtinguishingSystemPresent: string (nullable = true)\n",
      " |-- AutomaticExtinguishingSytemType: string (nullable = true)\n",
      " |-- AutomaticExtinguishingSytemPerfomance: string (nullable = true)\n",
      " |-- AutomaticExtinguishingSytemFailureReason: string (nullable = true)\n",
      " |-- NumberofSprinklerHeadsOperating: integer (nullable = true)\n",
      " |-- SupervisorDistrict: integer (nullable = true)\n",
      " |-- AnalysisNeighborhood: string (nullable = true)\n",
      " |-- point: string (nullable = true)\n",
      " |-- NeighborhoodsOld: integer (nullable = true)\n",
      " |-- ZipCodes: integer (nullable = true)\n",
      " |-- FirePreventionDistricts: integer (nullable = true)\n",
      " |-- PoliceDistricts: integer (nullable = true)\n",
      " |-- SupervisorDistricts: integer (nullable = true)\n",
      " |-- CivicCenterHarmReductionProjectBoundary: integer (nullable = true)\n",
      " |-- 2017FixItZones: integer (nullable = true)\n",
      " |-- HSOCZones: integer (nullable = true)\n",
      " |-- CentralMarketTenderloinBoundary: integer (nullable = true)\n",
      " |-- CentralMarketTenderloinBoundaryPolygonUpdated: integer (nullable = true)\n",
      " |-- HSOCZonesasof20180605: integer (nullable = true)\n",
      " |-- Neighborhoods: integer (nullable = true)\n",
      " |-- SFFindNeighborhoods: integer (nullable = true)\n",
      " |-- CurrentPoliceDistricts: integer (nullable = true)\n",
      " |-- CurrentSupervisorDistricts: integer (nullable = true)\n",
      " |-- AnalysisNeighborhoods: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "confident-future",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.select(\"ZIPCode\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "friendly-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=dataFrame.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "integrated-property",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "under-legend",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o216.parquet.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 291) (192.168.0.6 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: Mkdirs failed to create file:/D:/tempOutput/_temporary/0/_temporary/attempt_202107111609345586054665764078900_0015_m_000000_291/ZIPCode=9$%1A��m��mo�L�D��;�%25g�%3Fw��ŷ%19�%16��ovH0��a�5��%2A�ؒ��l͛�S�iy%01�r�O7����%25L%5D��%25��%1C�hk\u0000����%3Ev1�HB���%05���d%5C�(%13e%0Bo%1FIx�%3E3�%016BS%25���(%7F\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:317)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:401)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:464)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:149)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.newOutputWriter(FileFormatDataWriter.scala:241)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.write(FileFormatDataWriter.scala:262)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\t... 9 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\r\n\t... 33 more\r\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.IOException: Mkdirs failed to create file:/D:/tempOutput/_temporary/0/_temporary/attempt_202107111609345586054665764078900_0015_m_000000_291/ZIPCode=9$%1A��m��mo�L�D��;�%25g�%3Fw��ŷ%19�%16��ovH0��a�5��%2A�ؒ��l͛�S�iy%01�r�O7����%25L%5D��%25��%1C�hk\u0000����%3Ev1�HB���%05���d%5C�(%13e%0Bo%1FIx�%3E3�%016BS%25���(%7F\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:317)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:401)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:464)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:149)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.newOutputWriter(FileFormatDataWriter.scala:241)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.write(FileFormatDataWriter.scala:262)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\t... 9 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-3f5880419db9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ZIPCode\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Overwrite\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"D:/tempOutput\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\softwares\\python 3.6\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1247\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1249\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlineSep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\softwares\\python 3.6\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\softwares\\python 3.6\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\softwares\\python 3.6\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o216.parquet.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 291) (192.168.0.6 executor driver): org.apache.spark.SparkException: Task failed while writing rows.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: Mkdirs failed to create file:/D:/tempOutput/_temporary/0/_temporary/attempt_202107111609345586054665764078900_0015_m_000000_291/ZIPCode=9$%1A��m��mo�L�D��;�%25g�%3Fw��ŷ%19�%16��ovH0��a�5��%2A�ؒ��l͛�S�iy%01�r�O7����%25L%5D��%25��%1C�hk\u0000����%3Ev1�HB���%05���d%5C�(%13e%0Bo%1FIx�%3E3�%016BS%25���(%7F\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:317)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:401)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:464)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:149)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.newOutputWriter(FileFormatDataWriter.scala:241)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.write(FileFormatDataWriter.scala:262)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\t... 9 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\r\n\t... 33 more\r\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.IOException: Mkdirs failed to create file:/D:/tempOutput/_temporary/0/_temporary/attempt_202107111609345586054665764078900_0015_m_000000_291/ZIPCode=9$%1A��m��mo�L�D��;�%25g�%3Fw��ŷ%19�%16��ovH0��a�5��%2A�ؒ��l͛�S�iy%01�r�O7����%25L%5D��%25��%1C�hk\u0000����%3Ev1�HB���%05���d%5C�(%13e%0Bo%1FIx�%3E3�%016BS%25���(%7F\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:317)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:401)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:464)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:149)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.newOutputWriter(FileFormatDataWriter.scala:241)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.write(FileFormatDataWriter.scala:262)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\t... 9 more\r\n"
     ]
    }
   ],
   "source": [
    "dataFrame.write.partitionBy(\"ZIPCode\").mode(\"Overwrite\").parquet(\"D:/tempOutput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "affiliated-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write.partitionBy(\"ZIPCode\").mode(\"Overwrite\").parquet(\"D:/tempOutput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "excellent-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_1 = spark.read.parquet(\"D:/tempOutput/ZIPCode=94107/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "norman-temperature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- ExposureNumber: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- IncidentDate: string (nullable = true)\n",
      " |-- CallNumber: string (nullable = true)\n",
      " |-- AlarmDtTm: string (nullable = true)\n",
      " |-- ArrivalDtTm: string (nullable = true)\n",
      " |-- CloseDtTm: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- SuppressionUnits: integer (nullable = true)\n",
      " |-- SuppressionPersonnel: integer (nullable = true)\n",
      " |-- EMSUnits: integer (nullable = true)\n",
      " |-- EMSPersonnel: integer (nullable = true)\n",
      " |-- OtherUnits: integer (nullable = true)\n",
      " |-- OtherPersonnel: integer (nullable = true)\n",
      " |-- FirstUnitOnScene: string (nullable = true)\n",
      " |-- EstimatedPropertyLoss: integer (nullable = true)\n",
      " |-- EstimatedContentsLoss: string (nullable = true)\n",
      " |-- FireFatalities: string (nullable = true)\n",
      " |-- FireInjuries: string (nullable = true)\n",
      " |-- CivilianFatalities: integer (nullable = true)\n",
      " |-- CivilianInjuries: integer (nullable = true)\n",
      " |-- NumberofAlarms: integer (nullable = true)\n",
      " |-- PrimarySituation: string (nullable = true)\n",
      " |-- MutualAid: string (nullable = true)\n",
      " |-- ActionTakenPrimary: string (nullable = true)\n",
      " |-- ActionTakenSecondary: string (nullable = true)\n",
      " |-- ActionTakenOther: string (nullable = true)\n",
      " |-- DetectorAlertedOccupants: string (nullable = true)\n",
      " |-- PropertyUse: string (nullable = true)\n",
      " |-- AreaofFireOrigin: string (nullable = true)\n",
      " |-- IgnitionCause: string (nullable = true)\n",
      " |-- IgnitionFactorPrimary: string (nullable = true)\n",
      " |-- IgnitionFactorSecondary: string (nullable = true)\n",
      " |-- HeatSource: string (nullable = true)\n",
      " |-- ItemFirstIgnited: string (nullable = true)\n",
      " |-- HumanFactorsAssociatedwithIgnition: string (nullable = true)\n",
      " |-- StructureType: string (nullable = true)\n",
      " |-- StructureStatus: string (nullable = true)\n",
      " |-- FloorofFireOrigin: integer (nullable = true)\n",
      " |-- FireSpread: string (nullable = true)\n",
      " |-- NoFlameSpead: string (nullable = true)\n",
      " |-- Numberoffloorswithminimumdamage: integer (nullable = true)\n",
      " |-- Numberoffloorswithsignificantdamage: integer (nullable = true)\n",
      " |-- Numberoffloorswithheavydamage: integer (nullable = true)\n",
      " |-- Numberoffloorswithextremedamage: integer (nullable = true)\n",
      " |-- DetectorsPresent: string (nullable = true)\n",
      " |-- DetectorType: string (nullable = true)\n",
      " |-- DetectorOperation: string (nullable = true)\n",
      " |-- DetectorEffectiveness: string (nullable = true)\n",
      " |-- DetectorFailureReason: string (nullable = true)\n",
      " |-- AutomaticExtinguishingSystemPresent: string (nullable = true)\n",
      " |-- AutomaticExtinguishingSytemType: string (nullable = true)\n",
      " |-- AutomaticExtinguishingSytemPerfomance: string (nullable = true)\n",
      " |-- AutomaticExtinguishingSytemFailureReason: string (nullable = true)\n",
      " |-- NumberofSprinklerHeadsOperating: integer (nullable = true)\n",
      " |-- SupervisorDistrict: integer (nullable = true)\n",
      " |-- AnalysisNeighborhood: string (nullable = true)\n",
      " |-- point: string (nullable = true)\n",
      " |-- NeighborhoodsOld: integer (nullable = true)\n",
      " |-- ZipCodes: integer (nullable = true)\n",
      " |-- FirePreventionDistricts: integer (nullable = true)\n",
      " |-- PoliceDistricts: integer (nullable = true)\n",
      " |-- SupervisorDistricts: integer (nullable = true)\n",
      " |-- CivicCenterHarmReductionProjectBoundary: integer (nullable = true)\n",
      " |-- 2017FixItZones: integer (nullable = true)\n",
      " |-- HSOCZones: integer (nullable = true)\n",
      " |-- CentralMarketTenderloinBoundary: integer (nullable = true)\n",
      " |-- CentralMarketTenderloinBoundaryPolygonUpdated: integer (nullable = true)\n",
      " |-- HSOCZonesasof20180605: integer (nullable = true)\n",
      " |-- Neighborhoods: integer (nullable = true)\n",
      " |-- SFFindNeighborhoods: integer (nullable = true)\n",
      " |-- CurrentPoliceDistricts: integer (nullable = true)\n",
      " |-- CurrentSupervisorDistricts: integer (nullable = true)\n",
      " |-- AnalysisNeighborhoods: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "sustained-boundary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "imposed-nomination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
